[{"path":"https://mattplumlee.github.io/outerbase/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2022 Matthew Plumlee Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://mattplumlee.github.io/outerbase/articles/basebasics.html","id":"covariance-functions","dir":"Articles","previous_headings":"","what":"Covariance functions","title":"Base walkthrough","text":"Covariance functions important building block Gaussian process inference. package uses custom class represent covariance function. See ?covf information base class. Creating instances done ?methods::new call class name listed inside. cov method builds covariances matrices. example calling method. Note designed single dimensional covariance functions. Hyperparameters important almost covariance functions. control general shape behavior covariance function. stored covf class (editable) field hyp. can see effect alternating hyp correlation function.","code":"corf = new(covf_mat25) xred = x[1:5,1] print(corf$cov(xred,xred),3) #>       [,1]  [,2]  [,3]  [,4]  [,5] #> [1,] 1.000 1.000 0.999 0.998 0.997 #> [2,] 1.000 1.000 1.000 0.999 0.998 #> [3,] 0.999 1.000 1.000 1.000 0.999 #> [4,] 0.998 0.999 1.000 1.000 1.000 #> [5,] 0.997 0.998 0.999 1.000 1.000 corf$hyp #>      [,1] #> [1,]    0 corf$hyp = c(-0.5) plot(x[,1],corf$cov(x[,1],0.5), type='l',      ylab='correlation with 0.5', xlab='input') corf$hyp = c(-0.25) lines(x[,1],corf$cov(x[,1],0.5), type='l') corf$hyp = c(0) lines(x[,1],corf$cov(x[,1],0.5), type='l')"},{"path":"https://mattplumlee.github.io/outerbase/articles/basebasics.html","id":"gaussian-processes","dir":"Articles","previous_headings":"","what":"Gaussian processes","title":"Base walkthrough","text":"Gaussian processes long shown top performers near interpolation. information general Gaussian processes, see textbooks Gaussian Processes Machine Learning Surrogates, among others. idea represent surface realization Gaussian process controlled covariance function. outer product covariance functions can job three dimensions. means need first build covariance functions. multiply calculate covariance two sets points. goal Gaussian process inference take data predict number points, say 1000 points. predictor follows typical formulas (assuming y zero mean, see textbooks). gives prediction accuracy can summarized .  can also use framework get predictive variances. equations explained documentation brevity. One point pretty well! plot looks standard Normal enough.  main complaints Gaussian process inference stability computation speed. package designed reduce concerns.","code":"corf1 = new(covf_mat25) corf2 = new(covf_mat25) corf3 = new(covf_mat25) corf1$hyp = c(-0.5) # just setting them all to the same  corf2$hyp = c(-0.5) # hyperparameter for now corf3$hyp = c(-0.5) covftot = function(x1,x2){   corf1$cov(x1[,1],x2[,1])*   corf2$cov(x1[,2],x2[,2])*   corf3$cov(x1[,3],x2[,3]) } cormattot = covftot(x,x) #total correlation matrix testsampsize = 1000 xtest = matrix(runif(testsampsize*d),ncol=d) yhat = covftot(xtest,x) %*% solve(cormattot,y) ytest = obtest_borehole3d(xtest) plot(yhat, ytest, ylab=\"actual\", xlab=\"prediction\")  hist(ytest-yhat, main=\"test residuals\",      xlab = \"test residuals\") sigma2hat = as.double(t(y)%*% solve(cormattot,y)/length(y))  varpred = sigma2hat*(covftot(xtest,xtest)-t(covftot(x,xtest))%*%   solve(cormattot,covftot(x,xtest))) hist((ytest-yhat)/sqrt(diag(varpred)),      main=\"standarized test residuals\",      xlab = \"standarized test residuals\")"},{"path":"https://mattplumlee.github.io/outerbase/articles/basebasics.html","id":"outermod-and-outerbase","dir":"Articles","previous_headings":"","what":"outermod and outerbase","title":"Base walkthrough","text":"core classes package ?outermod ?outerbase. outermod instance contains information build outerbase instance, build objects corresponding specific x. outerbase instance used build inference specific x.","code":""},{"path":"https://mattplumlee.github.io/outerbase/articles/basebasics.html","id":"outermod","dir":"Articles","previous_headings":"outermod and outerbase","what":"outermod","title":"Base walkthrough","text":"instance class outermod designed hold information needed create basis matrix. outermod instance created using new command. first step set covariance functions knots. set vector covfs, use ?setcovfs alongside vector strings covariance functions package (?listcov). fixes dimension outermod instance om 3. need give set knot points dimension. choice still researched, choosing points spread dimension look like actual data currently recommended. need invert matrix size knot points, recommended keep small, <50 general. function ?setknot used. hyperparameters can set directly outermod object.","code":"om = new(outermod) setcovfs(om, rep(\"mat25\",3)) knotlist = list(seq(0,1,by=0.025),                 seq(0,1,by=0.025),                 seq(0,1,by=0.025)) setknot(om, knotlist) gethyp(om) #> inpt1.scale inpt2.scale inpt3.scale  #>           0           0           0 om$updatehyp(c(-0.5,-0.5,-0.5)) gethyp(om) #> inpt1.scale inpt2.scale inpt3.scale  #>        -0.5        -0.5        -0.5"},{"path":"https://mattplumlee.github.io/outerbase/articles/basebasics.html","id":"outerbase","dir":"Articles","previous_headings":"outermod and outerbase","what":"outerbase","title":"Base walkthrough","text":"instance class outerbase equivalent basis matrix fast computation methods included. also created new, also requires reference outermod instance specific set prediction points x. builds set basis functions dimension, sometimes just look like polynomials. quite polynomials, covariance functions give different shapes. call outerbase$getbase allow access basis functions dimension. mostly useful plotting.","code":"ob = new(outerbase,           om, # an outermod (reference only)           x) # an input matrix basis_func = ob$getbase(1) matplot(x[,1],basis_func[,1:4],          type='l', ylab=\"func\", xlab=\"first dim\")"},{"path":"https://mattplumlee.github.io/outerbase/articles/basebasics.html","id":"outermod-and-outerbase-1","dir":"Articles","previous_headings":"outermod and outerbase","what":"outermod and outerbase","title":"Base walkthrough","text":"outermod outerbase meant used conjunction . One key ingredient outermod$selectterms function, allows pick products basis functions best represent current outermod response. outermod$getvar returns vector variances associated coefficients terms. specific basis matrix can formed getting basis matrix selected terms. outerbase$getmat give short cut building matrix.","code":"p = 60 terms = om$selectterms(p) # 60 by 3 matrix head(terms) #>      [,1] [,2] [,3] #> [1,]    0    0    0 #> [2,]    1    0    0 #> [3,]    0    1    0 #> [4,]    0    0    1 #> [5,]    0    1    1 #> [6,]    1    1    0 covcoeff = as.vector(om$getvar(terms)) basismat = ob$getmat(terms)  termno = 5 basevec = ob$getbase(1)[,terms[termno,1]+1]*   ob$getbase(2)[,terms[termno,2]+1]*   ob$getbase(3)[,terms[termno,3]+1]  cbind(basevec[1:5],basismat[1:5,5]) # expect equal #>             [,1]        [,2] #> [1,] -0.02051294 -0.02051294 #> [2,] -0.05189621 -0.05189621 #> [3,]  0.07246694  0.07246694 #> [4,]  0.03052262  0.03052262 #> [5,] -1.19292749 -1.19292749"},{"path":"https://mattplumlee.github.io/outerbase/articles/basebasics.html","id":"feature-space-approximation","dir":"Articles","previous_headings":"","what":"Feature space approximation","title":"Base walkthrough","text":"package leverages insight Gaussian processes linear combinations basis functions random coefficients. viewpoint often called feature space view Gaussian processes. see , take covcoeff basismat together, correlation function well approximated following manipulation. means can leverage Bayesian linear regression prediction. require assuming noisevar, also called nugget Gaussian process literature.","code":"cormatob = basismat%*%diag(covcoeff)%*%t(basismat)  print(round(cormattot[1:5,1:5],3)) # typical gp #>       [,1]  [,2]  [,3]  [,4]  [,5] #> [1,] 1.000 0.947 0.931 0.964 0.756 #> [2,] 0.947 1.000 0.844 0.981 0.867 #> [3,] 0.931 0.844 1.000 0.861 0.751 #> [4,] 0.964 0.981 0.861 1.000 0.805 #> [5,] 0.756 0.867 0.751 0.805 1.000 print(round(cormatob[1:5,1:5],3)) # outerbase #>       [,1]  [,2]  [,3]  [,4]  [,5] #> [1,] 1.000 0.947 0.931 0.964 0.756 #> [2,] 0.947 1.000 0.844 0.981 0.867 #> [3,] 0.931 0.844 1.000 0.861 0.751 #> [4,] 0.964 0.981 0.861 1.000 0.805 #> [5,] 0.756 0.867 0.751 0.805 1.000 noisevar = 10^(-4) #posterior precision matrix of coefficients postcov = solve(1/noisevar*t(basismat)%*%basismat+                    1/sigma2hat*diag(1/covcoeff)) #posterior mean of coefficients coeffest = postcov%*%(1/noisevar*t(basismat)%*%y)"},{"path":"https://mattplumlee.github.io/outerbase/articles/basebasics.html","id":"predictions-and-comparison","dir":"Articles","previous_headings":"","what":"Predictions and comparison","title":"Base walkthrough","text":"Consider predicting new xtest examine inference works traditional Gaussian process feature space approximation. predictions nearly equivalent.  histograms residuals show similar matching.  standardized residuals, account variance, also show similar matching.","code":"obtest = new(outerbase,           om,     # same outermod            xtest) # new input matrix  basistest = obtest$getmat(terms) yhatob = basistest%*%coeffest plot(yhat, ytest, main=\"typical gp\",      xlab=\"prediction\", ylab=\"actual\") plot(yhatob, ytest, main = \"outerbase equiv.\",      xlab=\"prediction\", ylab=\"actual\") hist(ytest-yhat, main=\"typical gp\",      xlab=\"test residuals\") hist(ytest-yhatob, main=\"outerbase equiv.\",      xlab=\"test residuals\") varpredob = basistest%*%postcov%*%t(basistest)  hist((ytest-yhat)/sqrt(diag(varpred)), main=\"typical gp\",      xlab=\"standarized test residuals\") hist((ytest-yhatob)/sqrt(diag(varpredob)), main=\"outerbase equiv.\",      xlab=\"standarized test residuals\")"},{"path":"https://mattplumlee.github.io/outerbase/articles/gettingstarted.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Getting started","text":"goal outerbase make production near-interpolators easy, stable, scalable. based C++ backend interfaced R via Rcpp. hood, leverages unique, custom linear algebra using RcppArmadillo (Armadillo) omp. overall structure designed interacted object-oriented manner using Rcpp modules. ways interact outerbase uncomfortable (just actively detest) object oriented programming. begin, load package. Note built package source, make sure use compiler can process omp commands access entire speed benefits.","code":"library(outerbase)"},{"path":"https://mattplumlee.github.io/outerbase/articles/gettingstarted.html","id":"simple-prediction","dir":"Articles","previous_headings":"","what":"Simple prediction","title":"Getting started","text":"understand get started using package, predict using data function eight dimensional input commonly known Borehole function. begin generating 1000 points using test function ?obtest_borehole8d built package. goal design predictor y given x near-interpolator. simplest way interact package using ?obfit (fitting outerbase). function requires x, y two objects. value numb number basis functions want use. choice numb still research, generally want large tolerable. Play around! underlying concepts approach come Gaussian processes. Thus core building block predictors covariances functions. choice covariances needed obfit list strings corresponding column x. Type listcov() discover covariance functions currently deployed. curious , type, e.g. ?covf_mat25pow. Note obfit checks place prevent serious damage. foolproof. one correct deployment, mat25pow used dimensions. take bit run, around second modern computers. Note package made using custom parallelization linear-algebra level. package relies omp parallelization, package compiled place benefits. default call obfit grabs available threads, ideal desktops/laptops. might less ideal large clusters CPU might shared. can adjust number threads manually. reduce single thread, slow things . can predict using ?obpred. exact interpolator, close.  Since generated data, can show outerbase can reasonably predict ground truth, meaning overfitting issue.  1000 test points generated way original data can also serve verification process. predictions new points also quite good. quite good residuals test set, extrapolating .  package also produces variances predictions can use test reasonableness. fact second histogram looks like standard Normal promising predictions reasonable.","code":"sampsize = 400 d = 8 x = matrix(runif(sampsize*d),ncol=d) #uniform samples y = obtest_borehole8d(x) + 0.5*rnorm(sampsize) listcov() #> [1] \"mat25pow\" \"mat25\"    \"mat25ang\" obmodel = obfit(x, y, covnames=rep(\"elephant\",8)) #> Error in .checkcov(covnames[k], x[, k]):  #>  covariances must be from listcov() obmodel = obfit(x, y[1:200], covnames=rep(\"mat25pow\",5)) #> Error in obfit(x, y[1:200], covnames = rep(\"mat25pow\", 5)):  #>  x and y dims do not align obmodel = obfit(x[1:2,], y[1:2], covnames=rep(\"mat25pow\",8)) #> Error in obfit(x[1:2, ], y[1:2], covnames = rep(\"mat25pow\", 8)):  #>  dimension larger than sample size has not been tested obmodel = obfit(x, y, numb = 2, covnames=rep(\"mat25pow\",8)) #> Error in obfit(x, y, numb = 2, covnames = rep(\"mat25pow\", 8)):  #>  number of basis functions should be less than twice the dimension obmodel = obfit(100*x, y, covnames=rep(\"mat25pow\",8)) #> Error in .checkcov(covnames[k], x[, k]):  #>  x ranges exceed limits of covariance functions  #>  the limits are between 0 and 1   #>  try rescaling obmodel = obfit(0.001*x, y, covnames=rep(\"mat25pow\",8)) #> Error in .checkcov(covnames[k], x[, k]):  #>  x are too small for ranges #>  the limits are between 0 and 1   #>  try rescaling ptm = proc.time() obmodel = obfit(x, y, numb=300, covnames=rep(\"mat25pow\",8),                 verbose = 3)  #> doing partial optimization   #> max number of cg steps set to 100  #>  #> ########started BFGS####### #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        0    219.59           NA           NA           0.1 #> restarted hessian #> restarted hessian #> num iter: 3  obj start: 219.5902  obj end: 219.5896 #> final learning rate: 1e-04 #> approx lower bound (not achieved): -312.7595 #> #########finished BFGS########  #>  #> doing optimization  1  #> max number of cg steps set to 254  #>  #> ########started BFGS####### #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        0   625.485           NA           NA         5e-05 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        1   520.651     -104.823      7999.61        0.0016 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        2   356.469     -164.164     -47184.5   0.000380731 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        3  0.356744      -356.08     -315.781     0.0267736 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        4  -12.5289      -12.876       -34683    0.00120167 #> restarted hessian #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        6  -72.0436     -59.5097     -26.0728      0.037664 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        7  -158.492     -86.4404     -21.4396       0.10456 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        8  -245.959     -87.4571     -122.123      0.131047 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        9    -281.4     -35.4369     -15.7983      0.160578 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       10  -315.904     -34.5001     -13.3045      0.192804 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       11  -354.678     -38.7699     -17.6158      0.227303 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       12  -397.337     -42.6538     -19.3518      0.263599 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       13  -446.317     -48.9744     -25.6992      0.301197 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       14  -497.983     -51.6602     -39.2992      0.339598 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       15  -540.852     -42.8637     -42.5791      0.378328 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       16  -581.199     -40.3415     -33.3437      0.416948 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       17  -660.317     -79.1067     -64.0902      0.455065 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       18  -703.331     -43.0067     -91.0784      0.123085 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       19  -747.847      -44.508     -84.7406       0.15177 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       20  -814.205     -66.3448     -270.509     0.0916298 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       21  -905.965     -91.7404     -296.101      0.116368 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       22  -934.631     -28.6576     -88.5978      0.144295 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       23   -956.78     -22.1429     -41.5365      0.175116 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       24  -977.859     -21.0747     -23.4543      0.208445 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       25   -1001.3     -23.4374     -20.0027      0.243833 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       26  -1028.53     -27.2264     -18.2935      0.280791 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       27  -1064.14     -35.6083     -22.7983       0.31882 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       28   -1103.9     -39.7461     -58.7581       0.35743 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       29  -1120.13     -16.2293     -15.2188      0.396161 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       30   -1134.3     -14.1639     -9.06699      0.434595 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       31  -1152.71     -18.4086     -15.6514      0.472363 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       32  -1171.49     -18.7817     -22.2915      0.509153 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       33  -1188.35     -16.8507     -18.3144      0.544708 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       34  -1209.03     -20.6788     -15.3827      0.578825 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       35  -1237.63     -28.6011     -19.1216      0.611354 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       36  -1260.89     -23.2561     -25.8212       0.64219 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       37  -1268.37      -7.4751     -6.67341       0.67127 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       38  -1273.67      -5.3034     -2.26442      0.698566 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       39  -1283.54     -9.86448     -7.66035       0.72408 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       40  -1293.49     -9.94741     -6.55674      0.747839 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       41  -1302.33     -8.84174      -6.4604      0.769887 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       42  -1306.47     -4.13322     -2.68941      0.790286 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>       43  -1308.31     -1.84852    -0.983125      0.809107 #> num iter: 44  obj start: 625.4848  obj end: -1309.971 #> final learning rate: 0.8264287 #> approx lower bound (not achieved): -1312.51 #> #########finished BFGS########  #>  #> doing optimization  2  #> max number of cg steps set to 314  #>  #> ########started BFGS####### #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        0  -1307.21           NA           NA      0.413214 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        1  -1308.91     -1.69874    -0.105866      0.413214 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        2  -1314.59     -5.67711     -2.69489      0.451396 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        3  -1320.46     -5.86965     -2.82939      0.488767 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        4  -1326.52     -6.05995     -3.22347      0.525039 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        5  -1332.22     -5.70483     -3.96333       0.55998 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        6  -1336.66     -4.43742     -3.45809      0.593411 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        7  -1339.95     -3.29032     -3.53154      0.625201 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        8  -1341.76     -1.80249     -2.93696      0.655266 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        9  -1342.68    -0.927648     -1.90767      0.683559 #> num iter: 10  obj start: -1307.211  obj end: -1343.709 #> final learning rate: 0.7100656 #> approx lower bound (not achieved): -1350.03 #> #########finished BFGS######## print((proc.time() - ptm)[3]) #> elapsed  #>  10.165 ptm = proc.time() obmodel = obfit(x, y, numb=300, covnames=rep(\"mat25pow\",8),                 nthreads=1) #optional input print((proc.time() - ptm)[3]) #> elapsed  #>  10.641 predtr = obpred(obmodel, x) rmsetr = sqrt(mean((y-predtr$mean)^2)) plot(predtr$mean, y,      main=paste(\"training \\n RMSE = \", round(rmsetr,3)),      xlab=\"prediction\", ylab = \"actual\") ytrue = obtest_borehole8d(x) rmsetr = sqrt(mean((ytrue-predtr$mean)^2)) plot(predtr$mean, ytrue,      main=paste(\"oracle \\n RMSE = \", round(rmsetr,3)),      xlab=\"prediction\", ylab=\"actual\") xtest = matrix(runif(1000*d),ncol=d) #prediction points ytest = obtest_borehole8d(xtest) + 0.5*rnorm(1000) predtest = obpred(obmodel, xtest)  rmsetst = sqrt(mean((ytest-predtest$mean)^2)) plot(predtest$mean, ytest,       main=paste(\"testing \\n RMSE = \", round(rmsetst,3)),      xlab=\"prediction\", ylab=\"actual\") hist((ytest-predtest$mean),      main=\"testing \\n  residuals\", xlab=\"residuals\") hist((ytest-predtest$mean)/sqrt(predtest$var),      main=\"testing \\n standarized residuals\",      xlab=\"standarized residuals\")"},{"path":"https://mattplumlee.github.io/outerbase/articles/learning.html","id":"hyperparameter-impact","dir":"Articles","previous_headings":"","what":"Hyperparameter impact","title":"Learning from data","text":"values covariance function hyperparameters extremely important successful near-interpolation surfaces. impact felt outerbase section designed illustrate . Consider first four basis functions plotted .  hyperparameters now changed way know change basis functions. Note ob$build required updating hyperparameters take effect ob know om updated. leads asymmetric basis function set first dimension power transform ?covf_mat25pow.","code":"sampsize = 30 design1d = seq(1/(2*sampsize),1-1/(2*sampsize),1/sampsize) x = cbind(design1d,sample(design1d),sample(design1d)) ob = new(outerbase, om, x) basis_func0 = ob$getbase(1) matplot(x[,1],basis_func0[,1:4],          type='l', ylab=\"func\", xlab=\"first dim\") hyp0 = gethyp(om) hyp0[2] = 3 #changing the power on first parameter om$updatehyp(hyp0) ob$build() #rebuild after updatehyp basis_func1 = ob$getbase(1) matplot(x[,1],basis_func0[,1:4],          type='l', ylab=\"func\", xlab=\"first dim\",         main=\"original hyperparameters\") matplot(x[,1],basis_func1[,1:4],          type='l', ylab=\"func\", xlab=\"first dim\",         main=\"new hyperparameters\")"},{"path":"https://mattplumlee.github.io/outerbase/articles/learning.html","id":"lpdf-for-learning","dir":"Articles","previous_headings":"","what":"lpdf for learning","title":"Learning from data","text":"core building block outerbase learning base class ?lpdf, log probability density functions. base class forms backbone behind learning using statistical models. Instances class allow us optimize coefficients, infer uncertainty learn hyperparameters covariance functions. small dataset can illustrate (almost) core concepts related lpdf. length hyperparameters 2 dimensions covariance function total 6 hyperparameters. use 60 terms build approximation. idea build loglik object represent log likelihood data given model coefficients. begin ?loglik_std, although model recommended speed reasons. can initialize check can get gradients respect coefficients, covariance hyperparameters, parameters lpdf object . reasonable statistical model also needs prior coefficients. tells us distribution expect coefficients. make handling two objects loglik logpr easier, ?lpdfvec class helpful tie objects together. share hyperparameter vector , need based outermod object. concatenate parameters. coefficients coeff considered ancillary parameters need optimized (something sophisticated, hint current research). class, easiest via lpdf$optnewton, takes single Newton step optimize coefficients. test data help illustrate prediction. can see predictive accuracy using ?predictor class automatically pulls correct information loglik design predictions.","code":"y = obtest_borehole3d(x) gethyp(om) #> inpt1.scale inpt1.power inpt2.scale inpt2.power inpt3.scale inpt3.power  #>           0           3           0           0           0           0 hyp0 = c(-0.5,0,-0.5,0,-0.5,0) om$updatehyp(hyp0) terms = om$selectterms(60) loglik = new(loglik_std, om, terms, y, x)  coeff0 = rep(0,loglik$nterms) loglik$update(coeff0) # update it to get gradients loglik$val #> [1] -155.5177 head(loglik$grad) # dim 60 for number of coeffients #>               [,1] #> [1,]  0.1992810221 #> [2,] -1.1081469180 #> [3,]  2.3189286692 #> [4,] -0.9604307116 #> [5,]  0.0003926856 #> [6,] -0.6586072053 logpr = new(logpr_gauss, om, terms) logpdf = new(lpdfvec, loglik, logpr) para0 = getpara(logpdf) para0 #> noisescale coeffscale  #>   3.194473   6.000000 para0[2] = 4 logpdf$updatepara(para0) getpara(logpdf) #> noisescale coeffscale  #>   3.194473   4.000000 logpdf$optnewton() testsampsize = 1000 xtest = matrix(runif(testsampsize*d),ncol=d) ytest = obtest_borehole3d(xtest) predt = new(predictor,loglik) predt$update(xtest) yhat = as.vector(predt$mean()) varpred = as.vector(predt$var())  plot(yhat,ytest, xlab=\"prediction\", ylab=\"actual\") hist((ytest-yhat)/sqrt(varpred),      main=\"standarized test residuals\",      xlab = \"standarized test residuals\")"},{"path":"https://mattplumlee.github.io/outerbase/articles/learning.html","id":"lpdf-and-hyperparameters","dir":"Articles","previous_headings":"","what":"lpdf and hyperparameters","title":"Learning from data","text":"main value approach automated pulling important gradients related covariance hyperparameters model parameters. allows us use custom functions learn hyperparameters give maximum predictive power. goal right now single point estimate hyperparameters. One careful keep good ranges, call make sure return -inf problem chosen hyperparameters. works querying objects check parameters reasonable. package provides custom deployment BFGS ?BFGS_std optimize functions like . just update parameters re-optimize, can check least must closer stationary point. steps can nicely wrapped another function ?BFGS_lpdf, simpler call result. Note things fully understood, numbers match exactly , quite close functionally . revised predictions built hyperparameter optimization find improved predictive accuracy nearly every category. can see much better alignment predictions actual alongside better plot standardized residuals (like standard normal distribution).","code":"logpdf$optnewton() logpdf$gradhyp    # dim 6 for all hyperparameter #>             [,1] #> [1,] -1.66016266 #> [2,]  0.08873594 #> [3,]  6.05036401 #> [4,]  0.68746995 #> [5,]  6.24316716 #> [6,]  0.73454703 logpdf$gradpara   # dim 2 since 2 parameters #>            [,1] #> [1,] -19.720770 #> [2,]  -2.101124 totobj = function(parlist) { #my optimization function for tuning   regpara = logpdf$paralpdf(parlist$para) # get regularization for lpdf   reghyp = om$hyplpdf(parlist$hyp) # get regularization for om   if(is.finite(regpara) && is.finite(reghyp)) { # if they pass     om$updatehyp(parlist$hyp)        # update hyperparameters     logpdf$updateom()             # update the outerbase inside     logpdf$updatepara(parlist$para)  # update parameter     logpdf$optnewton()            # do opt          gval = parlist #match structure     gval$hyp = -logpdf$gradhyp-om$hyplpdf_grad(parlist$hyp)     gval$para = -logpdf$gradpara-logpdf$paralpdf_grad(parlist$para)     list(val = -logpdf$val-reghyp-regpara, gval = gval)   } else list(val = Inf, gval = parlist) } parlist = list(para = getpara(logpdf), hyp = gethyp(om)) totobj(parlist) #> $val #> [1] 88.72985 #>  #> $gval #> $gval$para #>           [,1] #> [1,] 19.720770 #> [2,]  1.601124 #>  #> $gval$hyp #>              [,1] #> [1,]  -3.69698019 #> [2,]  -0.08873594 #> [3,] -11.40750686 #> [4,]  -0.68746995 #> [5,] -11.60031002 #> [6,]  -0.73454703 opth = BFGS_std(totobj, parlist, verbose=3) # #>  #> ########started BFGS####### #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        0   88.7299           NA           NA           0.1 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        1   86.1514     -2.57797     -43.4597           0.1 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        2    81.264     -4.88689    -0.079983       0.50357 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        3   60.6539     -20.6071     -38.6212      0.539329 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        4   54.3006     -6.35252     -3.08005      0.573679 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        5    42.925     -11.3741     -9.93084      0.606459 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        6   33.9679     -8.95597     -5.97417      0.637561 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        7   30.1243     -3.84269     -14.0821      0.666913 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        8   29.7803     -0.34363     -9.57807      0.694484 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        9   28.9164    -0.863748      -1.6474      0.720271 #> num iter: 10  obj start: 88.72985  obj end: 28.85903 #> final learning rate: 0.7442976 #> approx lower bound (not achieved): 28.76414 #> #########finished BFGS######## totobj(opth$parlist) #> $val #> [1] 28.85903 #>  #> $gval #> $gval$para #>            [,1] #> [1,] -0.6215026 #> [2,] -1.7226547 #>  #> $gval$hyp #>              [,1] #> [1,] -0.007682007 #> [2,] -0.137870748 #> [3,] -0.245009276 #> [4,] -0.132537418 #> [5,] -0.043965009 #> [6,] -0.078487669 opth = BFGS_lpdf(om, logpdf,                   parlist=parlist,                   verbose = 3, newt= TRUE)   #>  #> ########started BFGS####### #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        0   88.7299           NA           NA           0.1 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        1   86.1514     -2.57797     -43.4597           0.1 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        2    81.264     -4.88689    -0.079983       0.50357 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        3   60.6539     -20.6071     -38.6212      0.539329 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        4   54.3006     -6.35252     -3.08005      0.573679 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        5    42.925     -11.3741     -9.93084      0.606459 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        6   33.9679     -8.95597     -5.97417      0.637561 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        7   30.1243     -3.84269     -14.0821      0.666913 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        8   29.7803     -0.34363     -9.57807      0.694484 #>  iter.no obj.value wolfe.cond.1 wolfe.cond.2 learning.rate #>        9   28.9164    -0.863748      -1.6474      0.720271 #> num iter: 10  obj start: 88.72985  obj end: 28.85903 #> final learning rate: 0.7442976 #> approx lower bound (not achieved): 28.76414 #> #########finished BFGS######## predtt = new(predictor,loglik) predtt$update(xtest) yhat = as.vector(predtt$mean()) varpred = as.vector(predtt$var())  plot(ytest,yhat) hist((ytest-yhat)/sqrt(varpred), main=\"standarized test residuals\",      xlab = \"standarized test residuals\")"},{"path":"https://mattplumlee.github.io/outerbase/articles/speed.html","id":"different-models","dir":"Articles","previous_headings":"","what":"Different models","title":"Speeding up inference","text":"begin, lets use ?loglik_std represent slow approach. logpdf_slow can optimized using lpdf$optnewton. Newton’s method involves solving linear system, thus takes one step, expensive. ?loglik_gauss lpdf model designed speed. nice comparison loglik_gauss uses model loglik_std, approximations speed. logpdf_fast error try use optnewton. written never builds Hessian (hess code) matrix. instead suggested use lpdf$optcg (conjugate gradient) optimize coefficients fast version. aside, omp speed ups possible, need correctly compiled omp. One check call following. answer 1 multicore processor (modern processors), installation might incorrect. can manually set number threads lpdf objects.","code":"loglik_slow = new(loglik_std, om, terms, y, x)  logpr_slow = new(logpr_gauss, om, terms) logpdf_slow = new(lpdfvec, loglik_slow, logpr_slow) logpdf_slow$optnewton() loglik_fast = new(loglik_gauss, om, terms, y, x)  logpr_fast = new(logpr_gauss, om, terms) logpdf_fast = new(lpdfvec, loglik_fast, logpr_fast) logpdf_fast$optnewton() #> Error in eval(expr, envir, enclos): addition: incompatible matrix dimensions: 0x0 and 250x250 logpdf_fast$optcg(0.001,  # tolerance                   100)    # max epochs ob = new(outerbase, om, x)  ob$nthreads #> [1] 2 logpdf_slow$setnthreads(4) logpdf_fast$setnthreads(4)"},{"path":"https://mattplumlee.github.io/outerbase/articles/speed.html","id":"timing","dir":"Articles","previous_headings":"","what":"Timing","title":"Speeding up inference","text":"main cost fitting outerbase models hyperparameter optimization. difference logpdf_slow logpdf_fast apparent. Let’s save starting points (since share om) fairness. Test points verify predictions equally good either model, difference speed. use unsophisticated proc.time quick timing comparisons.","code":"parlist_slow = list(para = getpara(logpdf_slow), hyp = gethyp(om)) parlist_fast = list(para = getpara(logpdf_fast), hyp = gethyp(om)) xtest = matrix(runif(1000*d),ncol=d) #prediction points ytest =  obtest_borehole8d(xtest) ptm = proc.time() opth = BFGS_lpdf(om, logpdf_slow,                   parlist=parlist_slow, newt=TRUE)     t_slow = proc.time() - ptm pred_slow = new(predictor,loglik_slow) pred_slow$update(xtest) yhat_slow = as.vector(pred_slow$mean()) print(t_slow) #>    user  system elapsed  #>   5.141   1.307   3.255 ptm = proc.time() opth = BFGS_lpdf(om, logpdf_fast,                   parlist=parlist_fast, newt=FALSE)   t_fast = proc.time() - ptm pred_fast = new(predictor,loglik_fast) pred_fast$update(xtest) yhat_fast = as.vector(pred_fast$mean()) print(t_fast) #>    user  system elapsed  #>   4.281   1.468   2.945"},{"path":"https://mattplumlee.github.io/outerbase/articles/speed.html","id":"comparison-of-results","dir":"Articles","previous_headings":"","what":"Comparison of results","title":"Speeding up inference","text":"simply plotting results tells story: faster inference discernible drop quality. Note serious approximations , approximations just negligible effect.","code":"rmse_slow = sqrt(mean((ytest-yhat_slow)^2)) hist((ytest-yhat_slow), main=paste(\"slow method \\n rmse:\",                                      round(rmse_slow,3),                                    \", time:\",                                    round(t_slow[3],2),'s'),      xlab = \"prediction residuals\") rmse_fast = sqrt(mean((ytest-yhat_fast)^2)) hist((ytest-yhat_fast), main=paste(\"fast method \\n rmse =\",                                       round(rmse_fast,3),                                    \", time:\",                                    round(t_fast[3],2),'s'),       xlab = \"prediction residuals\")"},{"path":"https://mattplumlee.github.io/outerbase/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Matthew Plumlee. Author, maintainer.","code":""},{"path":"https://mattplumlee.github.io/outerbase/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Plumlee M (2023). outerbase: Outer Product Regression. https://mattplumlee.github.io/outerbase/, https://github.com/MattPlumlee/outerbase/.","code":"@Manual{,   title = {outerbase: Outer Product Regression},   author = {Matthew Plumlee},   year = {2023},   note = {https://mattplumlee.github.io/outerbase/, https://github.com/MattPlumlee/outerbase/}, }"},{"path":"https://mattplumlee.github.io/outerbase/index.html","id":"outerbase","dir":"","previous_headings":"","what":"Outer Product Regression","title":"Outer Product Regression","text":"outerbase R package providing regression approaches designed creating emulators high-accuracy simulations. package creates high-dimensional approximations (near-interpolators) using unique outer product basis function structure. advantages similar approaches efficiency robustness. can used construct predictors stable consistent, remain accurate massive data, leverage large parallel computing resources, accommodate flexible data generation. software open source, can found Github, licensed MIT license. details installation references papers, see webpage docs. CRAN package now ! means outerbase can now installed directly using ?install.packages. development purposes, Github reliable way try code. code can pulled directly using devtools. project originated actively maintained Matthew Plumlee ( mplumlee@northwestern.edu)","code":"install.packages(\"outerbase\") devtools::install_github(\"mattplumlee/outerbase\")"},{"path":"https://mattplumlee.github.io/outerbase/reference/BFGS_lpdf.html","id":null,"dir":"Reference","previous_headings":"","what":"BFGS lpdf — BFGS_lpdf","title":"BFGS lpdf — BFGS_lpdf","text":"wrapper codeBFGS_std useful easily calling  parameter optimization package lines possible. Note om logpdf set optimal  parameters, return simply information.","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/BFGS_lpdf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BFGS lpdf — BFGS_lpdf","text":"","code":"BFGS_lpdf(   om,   logpdf,   parlist = list(),   newt = FALSE,   cgsteps = 100,   cgtol = 0.001,   ... )"},{"path":"https://mattplumlee.github.io/outerbase/reference/BFGS_lpdf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BFGS lpdf — BFGS_lpdf","text":"om outermod instance logpdf lpdf instance parlist initial point, pulled `om` `logpdf`  provided newt boolean Newtons method used cgsteps max number cg iterations, newt=FALSE cgtol cg tolerance, newt=FALSE ... additional parameters passed BFGS_std","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/BFGS_lpdf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BFGS lpdf — BFGS_lpdf","text":"list information optimization.","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/BFGS_std.html","id":null,"dir":"Reference","previous_headings":"","what":"BFGS standard — BFGS_std","title":"BFGS standard — BFGS_std","text":"generic minimization function funcw takes list parlist using \"Broyden-Fletcher-Goldfarb-Shanno\" (BFGS) algorithm. Useful hyperparameter optimization handles infinite returns fairly easily.","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/BFGS_std.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BFGS standard — BFGS_std","text":"","code":"BFGS_std(funcw, parlist, B = NULL, lr = 0.1, ..., verbose = 0)"},{"path":"https://mattplumlee.github.io/outerbase/reference/BFGS_std.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BFGS standard — BFGS_std","text":"funcw object optimize parlist initial point list B initial Hessian start lr initial learning rate start ... additional parameters passed funcw verbose integer 0-3 larger prints information","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/BFGS_std.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BFGS standard — BFGS_std","text":"list information optimization, value stored parlist","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/covf.html","id":null,"dir":"Reference","previous_headings":"","what":"covariance function class — covf","title":"covariance function class — covf","text":"base class designed handle specific features  covariances needed outerbase.  Polymorphism allows implied  methods used across several similar classes.","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/covf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"covariance function class — covf","text":"returns, class contains methods","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/covf.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"covariance function class — covf","text":"covf$hyp hyperparameters specific correlation function covf$lowbnd,covf$uppbnd upper lower bounds inputs  covariance function. covf$cov(x1,x2) returns covariance matrix two vectors  inputs x1 x2 covf$covdiag(x1) returns diagonal covariance matrix  x1 covf$cov_gradhyp(x1,x2) returns cube gradient cov  respect covariance hyperparameters","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/covf_mat25.html","id":null,"dir":"Reference","previous_headings":"","what":"Matern covariance function — covf_mat25","title":"Matern covariance function — covf_mat25","text":"standard Matern covariance function form $$c(x_1,x_2) = (1+ |h| + h^2/3) \\exp(-|h|) $$ \\(h = (x_1-x_2)/\\rho\\) \\(\\rho\\)=exp(2*hyp[0]).","code":"covf = new(covf_mat25)"},{"path":"https://mattplumlee.github.io/outerbase/reference/covf_mat25.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matern covariance function — covf_mat25","text":"returns, class contains methods","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/covf_mat25ang.html","id":null,"dir":"Reference","previous_headings":"","what":"Matern covariance function with angular transform — covf_mat25ang","title":"Matern covariance function with angular transform — covf_mat25ang","text":"standard Matern covariance function power transformation form $$c(x_1,x_2) = (1+ |h| + h^2/3) \\exp(-|h|) $$ $$h = \\sqrt{(\\sin(x_1)-\\sin(x_2))^2/\\rho_s +  (\\cos(x_1)-\\cos(x_2))^2/\\rho_c}.$$ hyp two dimensional vector  \\(\\rho_s\\)=exp(2*hyp[0]) \\(\\rho_c\\)=exp(2*hyp[1]).","code":"covf = new(covf_mat25ang)"},{"path":"https://mattplumlee.github.io/outerbase/reference/covf_mat25ang.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matern covariance function with angular transform — covf_mat25ang","text":"returns, class contains methods","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/covf_mat25pow.html","id":null,"dir":"Reference","previous_headings":"","what":"Matern covariance function with power transform — covf_mat25pow","title":"Matern covariance function with power transform — covf_mat25pow","text":"standard Matern covariance function power transformation form $$c(x_1,x_2) = (1+ |h| + h^2/3) \\exp(-|h|) $$ \\(h = (x_1^\\alpha-x_2^\\alpha)/\\rho\\) hyp two  dimensional vector \\(\\rho\\)=exp(2*hyp[0]+0.25*hyp[1]) \\(\\alpha\\)=exp(0.25*hyp[1]).","code":"covf = new(covf_mat25pow)"},{"path":"https://mattplumlee.github.io/outerbase/reference/covf_mat25pow.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matern covariance function with power transform — covf_mat25pow","text":"returns, class contains methods","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/gethyp.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the hyperparameters — gethyp","title":"Get the hyperparameters — gethyp","text":"Gets current hyperparameters outermod instance.  formats way makes reading R easier.","code":"hyp = gethyp(om)"},{"path":"https://mattplumlee.github.io/outerbase/reference/gethyp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the hyperparameters — gethyp","text":"om outermod instance","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/gethyp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the hyperparameters — gethyp","text":"vector parameters","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/gethyp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the hyperparameters — gethyp","text":"","code":"om = new(outermod) setcovfs(om, c(\"mat25\", \"mat25\", \"mat25\")) hyp = gethyp(om) print(hyp) #> inpt1.scale inpt2.scale inpt3.scale  #>           0           0           0"},{"path":"https://mattplumlee.github.io/outerbase/reference/getpara.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the model parameters — getpara","title":"Get the model parameters — getpara","text":"function gets current parameters lpdf class instance. formats way makes reading R easier.","code":"para = getpara(logpdf)"},{"path":"https://mattplumlee.github.io/outerbase/reference/getpara.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the model parameters — getpara","text":"logpdf lpdf class instance","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/getpara.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the model parameters — getpara","text":"vector parameters","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/listcov.html","id":null,"dir":"Reference","previous_headings":"","what":"list all covariance functions — listcov","title":"list all covariance functions — listcov","text":"list covariance functions","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/listcov.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"list all covariance functions — listcov","text":"","code":"listcov()"},{"path":"https://mattplumlee.github.io/outerbase/reference/listcov.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"list all covariance functions — listcov","text":"list names covariance functions recommend edition.  first default.","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/loglik_gauss.html","id":null,"dir":"Reference","previous_headings":"","what":"Gaussian errors, large scale — loglik_gauss","title":"Gaussian errors, large scale — loglik_gauss","text":"standard model form $$y = \\langle \\phi(x), \\theta \\rangle + \\varepsilon, \\varepsilon \\sim  N(0,\\sigma^2)$$ \\(\\phi(x)\\) basis, \\(\\theta\\) coefficient vector, \\(\\varepsilon\\) unseen noise vector.  parameter vector length 1  para \\(= \\log(\\sigma)\\).  faster (sometimes) version loglik_std  can handle diagonal variational  inference.","code":"loglik = new(loglik_gauss, om, terms, y, x)"},{"path":"https://mattplumlee.github.io/outerbase/reference/loglik_gauss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gaussian errors, large scale — loglik_gauss","text":"om outermod instance referred terms matrix terms, must many columns dims  om y vector observations x matrix predictors, must many columns dims  om number rows y","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/loglik_gauss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Gaussian errors, large scale — loglik_gauss","text":"returns, class contains methods","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/loglik_gda.html","id":null,"dir":"Reference","previous_headings":"","what":"Gaussian errors with diagonal adjustment — loglik_gda","title":"Gaussian errors with diagonal adjustment — loglik_gda","text":"standard model form $$y = \\langle \\phi(x), \\theta \\rangle + \\delta(x) + \\varepsilon, \\delta(x) \\sim N(0, \\lambda g(x)), \\varepsilon \\sim N(0,\\sigma^2)$$ \\(\\phi(x)\\) basis, \\(\\theta\\) coefficient vector, \\(\\delta(x)\\) unseen vector corresponding unmodeled  variance \\(\\lambda g(x)\\), \\(\\varepsilon\\) unseen noise vector. parameter vector length 2  \\(\\sigma=\\) exp(para[0]) \\(\\lambda=\\)exp(2*para[1]).","code":"loglik = new(loglik_gda, om, terms, y, x)"},{"path":"https://mattplumlee.github.io/outerbase/reference/loglik_gda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gaussian errors with diagonal adjustment — loglik_gda","text":"om outermod instance referred terms matrix terms, must many columns dims  om y vector observations x matrix predictors, must many columns dims  om number rows y","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/loglik_gda.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Gaussian errors with diagonal adjustment — loglik_gda","text":"returns, class contains methods","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/loglik_std.html","id":null,"dir":"Reference","previous_headings":"","what":"Gaussian errors — loglik_std","title":"Gaussian errors — loglik_std","text":"standard model form $$y = \\langle \\phi(x), \\theta \\rangle + \\varepsilon, \\varepsilon \\sim  N(0,\\sigma^2)$$ \\(\\phi(x)\\) basis, \\(\\theta\\) coefficient vector, \\(\\varepsilon\\) unseen noise vector. parameter vector length 1  para \\(= \\log(\\sigma)\\).  slower (sometimes)  version loglik_gauss allows complete marginal  inference.","code":"loglik = new(loglik_std, om, terms, y, x)"},{"path":"https://mattplumlee.github.io/outerbase/reference/loglik_std.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gaussian errors — loglik_std","text":"om outermod instance referred terms matrix terms, must many columns dims  om y vector observations x matrix predictors, must many columns dims  om number rows y","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/loglik_std.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Gaussian errors — loglik_std","text":"returns, class contains methods","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/logpr_gauss.html","id":null,"dir":"Reference","previous_headings":"","what":"Gaussian prior — logpr_gauss","title":"Gaussian prior — logpr_gauss","text":"standard model coefficients drawn  independently $$ \\theta_i \\sim N(0, \\rho c_i)$$ \\(c_i\\) variance supplied om \\(\\)th term.  parameter vector length 1  \\(\\rho=\\) exp(para[0]).","code":"logpr = new(logpr_gauss, om, terms)"},{"path":"https://mattplumlee.github.io/outerbase/reference/logpr_gauss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gaussian prior — logpr_gauss","text":"om outermod instance referred terms matrix terms, must many columns dims  om","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/logpr_gauss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Gaussian prior — logpr_gauss","text":"returns, class contains methods","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/lpdf-cash-optcg.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimization via Conjugate Gradient — lpdf$optcg","title":"Optimization via Conjugate Gradient — lpdf$optcg","text":"optimizes coefficient vector coeff using conjugate gradient.  currently designed quadratic lpdf instances.","code":"lpdf$optcg(tol,epoch)"},{"path":"https://mattplumlee.github.io/outerbase/reference/lpdf-cash-optcg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimization via Conjugate Gradient — lpdf$optcg","text":"tol positive double representing tolerance, default  0.001. epoch positive integer representing maximum number steps  conjugate gradient take.","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/lpdf-cash-optcg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimization via Conjugate Gradient — lpdf$optcg","text":"nothing returned, class instance updated","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/lpdf-cash-optnewton.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimization via Newton's Method — lpdf$optnewton","title":"Optimization via Newton's Method — lpdf$optnewton","text":"optimizes coefficient vector coeff using Newton's Method.   currently designed quadratic lpdf instances.   take single step.","code":"lpdf$optnewton()"},{"path":"https://mattplumlee.github.io/outerbase/reference/lpdf-cash-optnewton.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimization via Newton's Method — lpdf$optnewton","text":"nothing returned, class instance updated","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/lpdf.html","id":null,"dir":"Reference","previous_headings":"","what":"Log probability density function class — lpdf","title":"Log probability density function class — lpdf","text":"base class designed handle learning  underlying coefficients, hyperparameters, parameters associated specific learning instance.  Polymorphism allows implied methods  used across several similar classes.","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/lpdf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Log probability density function class — lpdf","text":"returns, class contains methods","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/lpdf.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Log probability density function class — lpdf","text":"lpdf$val current value lpdf$para current model parameters lpdf$coeff current coefficients lpdf$compute_val calling update, compute value store  val lpdf$grad current gradient respect coefficients lpdf$gradhyp current gradient respect covariance hyperparameters lpdf$gradpara current gradient respect model parameters lpdf$compute_grad calling update, compute gradient  respect coefficients store grad lpdf$compute_gradhyp calling update, compute gradient respect covariance hyperparameters store gradhyp lpdf$compute_gradpara calling update, compute gradient respect model parameters store gradpara lpdf$update(coeff) update using new coefficients lpdf$optcg(tol,epoch) optimization respect coefficients  via conjugate gradient lpdf$optnewton() optimization via matrix inversion, one Newton  step lpdf$updateom() update based recent version outermod lpdf$updatepara(para) update using new model parameters lpdf$updateterms(terms) update using new terms lpdf$hess() returns hessian respect  coefficients lpdf$hessgradhyp() returns gradient hess() respect  covariance hyperparameters lpdf$hessgradpara() returns gradient hess() respect  model parameters lpdf$diaghess() returns diagonal hessian  respect coefficients lpdf$diaghessgradhyp() returns gradient diaghess()  respect  covariance hyperparameters lpdf$diaghessgradpara() returns gradient diaghess()  respect model parameters lpdf$paralpdf(para) compute log-prior parameters, useful  fitting lpdf$paralpdf_grad(para) gradient paralpdf(para)","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/lpdfvec.html","id":null,"dir":"Reference","previous_headings":"","what":"Vector of lpdf instances — lpdfvec","title":"Vector of lpdf instances — lpdfvec","text":"class instance contains two lpdf  instances can  manipulated single instance.  presumes based outermod instance, thus share hyperparameters.  However model parameters concatenated.  Currently also includes variations marginal adjustments. Currently designed pair, ordering arbitrary.","code":"logpdf = new(lpdfvec, loglik, logpr)"},{"path":"https://mattplumlee.github.io/outerbase/reference/lpdfvec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Vector of lpdf instances — lpdfvec","text":"loglik one reference lpdf instance logpr another reference lpdf instance shares  outermod  loglik","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/lpdfvec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Vector of lpdf instances — lpdfvec","text":"returns, class contains methods","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/lpdfvec.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Vector of lpdf instances — lpdfvec","text":"lpdfvec$domarg boolean controls marginal adjustment  done","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/obfit.html","id":null,"dir":"Reference","previous_headings":"","what":"Outerbase model fit — obfit","title":"Outerbase model fit — obfit","text":"function fits outerbase model prediction hides actual object-oriented aspects package.","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/obfit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Outerbase model fit — obfit","text":"","code":"obfit(   x,   y,   numb = 100,   verbose = 0,   covnames = NULL,   hyp = NULL,   numberopts = 2,   nthreads = NULL )"},{"path":"https://mattplumlee.github.io/outerbase/reference/obfit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Outerbase model fit — obfit","text":"x n d sized matrix inputs y n length vector outputs numb size basis use verbose 0-3, much information optimization print console covnames d length vector covariance names hyp initial covariance hyperparameters numberopts number optimizations done hyperparameters, must larger 1 nthreads number threads used learning","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/obfit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Outerbase model fit — obfit","text":"Saving important model information used  obpred","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/obpred.html","id":null,"dir":"Reference","previous_headings":"","what":"Prediction from outerbase — obpred","title":"Prediction from outerbase — obpred","text":"function allows turning obmodel predictions  mean variance.","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/obpred.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prediction from outerbase — obpred","text":"","code":"obpred(obmodel, x)"},{"path":"https://mattplumlee.github.io/outerbase/reference/obpred.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prediction from outerbase — obpred","text":"obmodel output obfit x new m d sized matrix inputs","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/obpred.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prediction from outerbase — obpred","text":"list mean var new x","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/obtest_borehole3d.html","id":null,"dir":"Reference","previous_headings":"","what":"Three dim borehole example — obtest_borehole3d","title":"Three dim borehole example — obtest_borehole3d","text":"three dimensional Borehole function used illustrations.","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/obtest_borehole3d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Three dim borehole example — obtest_borehole3d","text":"","code":"obtest_borehole3d(x)"},{"path":"https://mattplumlee.github.io/outerbase/reference/obtest_borehole3d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Three dim borehole example — obtest_borehole3d","text":"x n 3 vector inputs","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/obtest_borehole3d.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Three dim borehole example — obtest_borehole3d","text":"length n vector outputs","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/obtest_borehole8d.html","id":null,"dir":"Reference","previous_headings":"","what":"Eight dim borehole example — obtest_borehole8d","title":"Eight dim borehole example — obtest_borehole8d","text":"eight dimensional Borehole function used illustrations.","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/obtest_borehole8d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Eight dim borehole example — obtest_borehole8d","text":"","code":"obtest_borehole8d(x)"},{"path":"https://mattplumlee.github.io/outerbase/reference/obtest_borehole8d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Eight dim borehole example — obtest_borehole8d","text":"x n 8 vector inputs","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/obtest_borehole8d.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Eight dim borehole example — obtest_borehole8d","text":"length n vector outputs","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase-cash-build.html","id":null,"dir":"Reference","previous_headings":"","what":"Builds the outerbase — outerbase$build","title":"Builds the outerbase — outerbase$build","text":"Build (re-build) basis based recent evaluation  outermod.","code":"outerbase$build()"},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase-cash-build.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Builds the outerbase — outerbase$build","text":"nothing returned, class instance updated","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase-cash-getbase.html","id":null,"dir":"Reference","previous_headings":"","what":"Get base functions — outerbase$getbase","title":"Get base functions — outerbase$getbase","text":"Returns basis dimension k.   Designed mostly  visualization.","code":"basis_func = outerbase$getbase(k)"},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase-cash-getbase.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get base functions — outerbase$getbase","text":"k integer corresponds dimension.","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase-cash-getbase.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get base functions — outerbase$getbase","text":"matrix evaluated basis functions","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase-cash-getmat.html","id":null,"dir":"Reference","previous_headings":"","what":"Get basis matrix — outerbase$getmat","title":"Get basis matrix — outerbase$getmat","text":"Returns basis matrix given set terms.","code":"basismat = outerbase$getmat(terms)"},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase-cash-getmat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get basis matrix — outerbase$getmat","text":"terms matrix terms","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase-cash-getmat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get basis matrix — outerbase$getmat","text":"matrix evaluated basis functions based  terms.","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase-cash-matmul.html","id":null,"dir":"Reference","previous_headings":"","what":"Matrix multiply — outerbase$matmul","title":"Matrix multiply — outerbase$matmul","text":"Multiplies basis times vector without building basis  matrix.","code":"b = outerbase$matmul(terms, a)"},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase-cash-matmul.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matrix multiply — outerbase$matmul","text":"terms matrix terms vector length rows terms","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase-cash-matmul.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matrix multiply — outerbase$matmul","text":"vector resulting matrix multiplication","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase-cash-tmatmul.html","id":null,"dir":"Reference","previous_headings":"","what":"Transpose Matrix multiply — outerbase$tmatmul","title":"Transpose Matrix multiply — outerbase$tmatmul","text":"Multiplies transpose basis times vector without   building basis matrix.","code":"b = outerbase$tmatmul(terms, a)"},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase-cash-tmatmul.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transpose Matrix multiply — outerbase$tmatmul","text":"terms matrix terms vector length rows outerbase","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase-cash-tmatmul.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transpose Matrix multiply — outerbase$tmatmul","text":"vector resulting matrix multiplication","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase-package.html","id":null,"dir":"Reference","previous_headings":"","what":"outerbase — outerbase-package","title":"outerbase — outerbase-package","text":"High-dimensional regression using outer product models. Research methods currently investigation published resources posted available. method new, website best resource understanding principals. core ideas based Plumlee coauthors' work analysis grid-structured experiments described Plumlee (2014) doi:10.1080/01621459.2014.900250  Plumlee, Erickson, Ankenman, Lawrence (2021) doi:10.1093/biomet/asaa084 . additional textbooks additional information Gaussian processes Rasmussen Williams (2005) doi:10.7551/mitpress/3206.001.0001  Gramacy (2022) doi:10.1201/9780367815493 .","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"outerbase — outerbase-package","text":"Maintainer: Matthew Plumlee mplumlee@northwestern.edu","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase.html","id":null,"dir":"Reference","previous_headings":"","what":"Outer product-type basis — outerbase","title":"Outer product-type basis — outerbase","text":"Class handles basis given set points  x.","code":"ob = new(outerbase, om, x)"},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Outer product-type basis — outerbase","text":"x matrix predictors, must many columns dims  om","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Outer product-type basis — outerbase","text":"returns, class contains methods","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Outer product-type basis — outerbase","text":"nthreads number threads omp use outerbase$getbase(k) get dimensions basis  functions outerbase$getmat(terms) get basis matrix  terms outerbase$build() (re)build basis instance outerbase$matmul(terms,) matrix multiply without  building basis matrix outerbase$tmatmul(terms,) transpose matrix multiply  without building basis matrix","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/outerbase.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Outer product-type basis — outerbase","text":"","code":"om = new(outermod) setcovfs(om, c(\"mat25\", \"mat25\", \"mat25\")) setknot(om,          list(seq(0,1,by=0.025),seq(0,1,by=0.025),seq(0,1,by=0.025))) x = matrix(runif(10*3),ncol=3) ob = new(outerbase, om, x) terms = om$selectterms(40) basismat = ob$getmat(terms)"},{"path":"https://mattplumlee.github.io/outerbase/reference/outermod-cash-getvar.html","id":null,"dir":"Reference","previous_headings":"","what":"Get variance of coefficients — outermod$getvar","title":"Get variance of coefficients — outermod$getvar","text":"Returns variance coefficients associated terms.","code":"coeffvar = outermod$getvar(terms)"},{"path":"https://mattplumlee.github.io/outerbase/reference/outermod-cash-getvar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get variance of coefficients — outermod$getvar","text":"terms matrix terms","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/outermod-cash-getvar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get variance of coefficients — outermod$getvar","text":"vector variances coefficient","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/outermod-cash-selectterms.html","id":null,"dir":"Reference","previous_headings":"","what":"Select optimal terms — outermod$selectterms","title":"Select optimal terms — outermod$selectterms","text":"Returns best numterms given outermod currently using maximum variance criteria.","code":"terms = om$selectterms(numterms)"},{"path":"https://mattplumlee.github.io/outerbase/reference/outermod-cash-selectterms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select optimal terms — outermod$selectterms","text":"numterms number basis terms desired","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/outermod-cash-selectterms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select optimal terms — outermod$selectterms","text":"matrix terms","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/outermod-cash-updatehyp.html","id":null,"dir":"Reference","previous_headings":"","what":"Update hyperparameters — outermod$updatehyp","title":"Update hyperparameters — outermod$updatehyp","text":"Updates hyperparameters instance outermod.","code":"outermod$updatehyp(hyp)"},{"path":"https://mattplumlee.github.io/outerbase/reference/outermod-cash-updatehyp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update hyperparameters — outermod$updatehyp","text":"hyp vector hyperparameters","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/outermod-cash-updatehyp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update hyperparameters — outermod$updatehyp","text":"value returned, class instance updated","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/outermod.html","id":null,"dir":"Reference","previous_headings":"","what":"Outer product-type model — outermod","title":"Outer product-type model — outermod","text":"class used construct outerbase class instances.  stores key information constructing basis.","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/outermod.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Outer product-type model — outermod","text":"returns, class contains methods","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/outermod.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Outer product-type model — outermod","text":"outermod$updatehyp(hyp) update hyperparameters outermod$selectterms(numterms) find best numterms terms outermod$getvar(terms) find variances coefficients  associated  terms","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/outermod.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Outer product-type model — outermod","text":"","code":"om = new(outermod) setcovfs(om, c(\"mat25\", \"mat25\", \"mat25\")) setknot(om,          list(seq(0,1,by=0.01),seq(0,1,by=0.01),seq(0,1,by=0.01))) terms = om$selectterms(40) coeffvar =om$getvar(terms) hyp = gethyp(om) hyp[1:2] = 0.5 om$updatehyp(hyp) coeffvar = om$getvar(terms)"},{"path":"https://mattplumlee.github.io/outerbase/reference/predictor.html","id":null,"dir":"Reference","previous_headings":"","what":"prediction class — predictor","title":"prediction class — predictor","text":"base class design allow coherent building predictions across multiple models.  Unlike many base classes  package, meant directly used.","code":"pred = new(predictor, loglik)"},{"path":"https://mattplumlee.github.io/outerbase/reference/predictor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"prediction class — predictor","text":"loglik lpdf instance, specifically starts  loglik, build predictor","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/predictor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"prediction class — predictor","text":"returns, class contains methods","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/predictor.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"prediction class — predictor","text":"predictor$update(x) update current input x prediction predictor$mean() return vector means prediction predictor$var() return vector variances prediction predictor$setnthreads(k) specifics k number threads use","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/setcovfs.html","id":null,"dir":"Reference","previous_headings":"","what":"Set covariance functions — setcovfs","title":"Set covariance functions — setcovfs","text":"Sets covariance functions outermod class instance. first thing one creating outermod instance.","code":"setcovfs(om, covnames)"},{"path":"https://mattplumlee.github.io/outerbase/reference/setcovfs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set covariance functions — setcovfs","text":"om outermod instance covnames vector strings covariance functions","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/setcovfs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set covariance functions — setcovfs","text":"value returned, om updated","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/setcovfs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set covariance functions — setcovfs","text":"","code":"om = new(outermod) setcovfs(om, c(\"mat25\", \"mat25\", \"mat25\")) setcovfs(om, c(\"mat25\", \"mat25pow\", \"mat25\", \"mat25ang\"))"},{"path":"https://mattplumlee.github.io/outerbase/reference/setknot.html","id":null,"dir":"Reference","previous_headings":"","what":"Set knot points — setknot","title":"Set knot points — setknot","text":"Sets knot points om knotslist estimate  eigenfunctions eigenvalues. naturally check knot points  dimension covariance functions.  also check  knot points within reasonable bounds covariance functions.","code":"setknot(om, knotslist)"},{"path":"https://mattplumlee.github.io/outerbase/reference/setknot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set knot points — setknot","text":"om outermod instance knotslist list one dimensional vectors","code":""},{"path":"https://mattplumlee.github.io/outerbase/reference/setknot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set knot points — setknot","text":"value returned, om updated","code":""},{"path":[]},{"path":"https://mattplumlee.github.io/outerbase/reference/setknot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set knot points — setknot","text":"","code":"om = new(outermod) setcovfs(om, c(\"mat25\", \"mat25\", \"mat25\")) knotslist = list(seq(0,1,by=0.01),seq(0,1,by=0.01),seq(0,1,by=0.01)) setknot(om, knotslist)"},{"path":"https://mattplumlee.github.io/outerbase/news/index.html","id":"outerbase-010","dir":"Changelog","previous_headings":"","what":"outerbase 0.1.0","title":"outerbase 0.1.0","text":"CRAN release: 2022-06-09 Initial release! Contains base functionality.","code":""}]
