---
title: "Base walkthrough"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Base walkthrough}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(outerbase)
```

A three dimensional case study is sufficient for understanding what `?outermod` and `?outerbase` are and how to manipulate them.

```{r}
sampsize = 30
d = 3
design1d = seq(1/(2*sampsize),1-1/(2*sampsize),1/sampsize)
x = cbind(design1d,sample(design1d),sample(design1d))
y = obtest_borehole3d(x)
```

## Covariance functions
Covariance functions are an important building block of Gaussian process inference.  In this package, they flexible classes.  See `?covf` for more information on the base class.

Creating instances is done through the `new` call with the class name listed inside.
```{r}
corf = new(covf_mat25)
?covf_mat25
```

The `cov` method is what builds covariances matrices.  Below the covariance between the first columns first 5 elements is ploted.  Note that these are designed to be single demensional covariance functions.
```{r}
xred = x[1:5,1]
print(corf$cov(xred,xred),3)
```
Hyperparameters are important to almost all covariance functions. They control the general shape of the predictive surface.  They are stored in the `covf` class in the (editable) field `hyp`.
```{r}
corf$hyp
```

You can see the effect of alternating `hyp` below on this correlation function.
```{r}
corf$hyp = c(-0.5)
plot(x[,1],corf$cov(x[,1],0.5), type='l',
     ylab='correlation with 0.5', xlab='input')
corf$hyp = c(-0.25)
lines(x[,1],corf$cov(x[,1],0.5), type='l')
corf$hyp = c(0)
lines(x[,1],corf$cov(x[,1],0.5), type='l')
```

## Gaussian processes

Gaussian processes have long been shown to be top performers for near interpolation.  For more information on general Gaussian processes, see the textbooks [Gaussian Processes for Machine Learning](https://gaussianprocess.org/gpml/) or [Surrogates](https://bobby.gramacy.com/surrogates/), among others.

The idea is to represent a surface as a realization of a Gaussian process.  To do these in three dimensions, one of the optimal ways is through an outer product of covariance functions.  This means we need to first build correlation functions.
```{r}
corf1 = new(covf_mat25)
corf2 = new(covf_mat25)
corf3 = new(covf_mat25)
corf1$hyp = c(-0.5) # just setting them all to the same 
corf2$hyp = c(-0.5) # hyperparameter for now
corf3$hyp = c(-0.5)
```

And then multiply them to calculate covariances.
```{r}
covftot = function(x1,x2){
  corf1$cov(x1[,1],x2[,1])*
  corf2$cov(x1[,2],x2[,2])*
  corf3$cov(x1[,3],x2[,3])
}
cormattot = covftot(x,x) #total correlation matrix
```

If we want to predict at some set of points, say `1000` points.
```{r}
testsampsize = 1000
xtest = matrix(runif(testsampsize*d),ncol=d)
```

Then we built the predictor (assuming y is zero mean) as
```{r}
yhat = covftot(xtest,x) %*% solve(cormattot,y)
```

This gives prediction accuracy that can be summarized below.
```{r}
ytest = obtest_borehole3d(xtest)
plot(ytest,yhat, xlab="actual", ylab="prediction") 
hist(ytest-yhat, main="test residuals",
     xlab = "test residuals")
```
We can also use this framework to get predictive variances.  These equations will not be explained in this documentation for brevity.  One point here is that is does pretty well!  The second plot looks standard Normal enough.
```{r}
sigma2hat = as.double( t(y) %*% solve(cormattot,y) / length(y))

varpred = sigma2hat * (covftot(xtest,xtest) - t(covftot(x,xtest)) %*% 
  solve(cormattot,covftot(x,xtest)))
hist((ytest-yhat)/sqrt(diag(varpred)), main="standarized test residuals",
     xlab = "standarized test residuals")
```


## mod and base

### outermod

```{r}
om = new(outermod)
```

```{r}
setcovfs(om, rep("mat25",3))

knotlist = list(seq(0,1,by=0.025),
                seq(0,1,by=0.025),
                seq(0,1,by=0.025))
setknot(om, knotlist)
```

```{r}
gethyp(om)
om$updatehyp(rep(-0.5,3))
gethyp(om)
```

### outerbase

`outerbase` is the equivalent of a basis matrix with 
fast computation built in
```{r}
ob = new(outerbase, 
         om, #give it the outermod (reference)
          x) #give it the input matrix
```

```{r}
basis_func = ob$getbase(1)
matplot(x[,1],basis_func[,1:4], 
        type='l', ylab="func", xlab="first dim")
```

### outermod and outerbase

```{r}
p = 60
terms = om$selectterms(p)
print(terms)
```


```{r}
basismat = ob$getmat(terms)


basevec = ob$getbase(1)[,terms[5,1]+1]*
  ob$getbase(2)[,terms[5,2]+1]*
  ob$getbase(3)[,terms[5,3]+1]
cbind(basevec[1:5],basismat[1:5,5])
```

```{r}
covcoeff = as.vector(om$getvar(terms))
```

## outerbase inference

```{r}
cormatob = basismat %*% diag(covcoeff ) %*% t(basismat)

print(round(cormatob[1:5,1:5],4))
print(round(cormattot[1:5,1:5],4))
```


```{r}
cormatob = basismat %*% diag(covcoeff) %*% t(basismat)

paraep = 10^(-2)
postcov = solve(1/paraep * t(basismat) %*% basismat + 
                  1/sigma2hat*diag(1/covcoeff))
coeffest = postcov %*% (1/paraep * t(basismat) %*% y)
```


```{r}
paraep = 10^(-2)
postcov = solve(1/paraep * t(basismat) %*% basismat + 
                  1/sigma2hat*diag(1/covcoeff))
coeffest = postcov %*% (1/paraep * t(basismat) %*% y)

```

## Predictions and comparison

```{r}
obtest = new(outerbase, 
         om, #give it the outermod (reference)
          xtest) #give it the input matrix

basistest = obtest$getmat(terms)
```

```{r}
yhatob = basistest %*% coeffest
par(mfrow=c(1,2))
plot(yhat,ytest, main = "typical gp")
plot(yhatob,ytest, main = "outerbase equiv.")
```

```{r}
par(mfrow=c(1,2))
hist(ytest-yhat, main="typical gp",
     xlab = "test residuals")
hist(ytest-yhatob, main="outerbase equiv.",
     xlab = "test residuals")
```

```{r}
varpredob = basistest %*% postcov %*% t(basistest) 
par(mfrow=c(1,2))
hist((ytest-yhat)/sqrt(diag(varpred)), main="typical gp",
     xlab = "standarized test residuals")
hist((ytest-yhatob)/sqrt(diag(varpredob)), main="outerbase equiv.",
     xlab = "standarized test residuals")
```

